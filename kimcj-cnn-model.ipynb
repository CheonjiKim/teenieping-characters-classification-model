{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 모델 만들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 데이터는 다음과 같은 구조로 되어 있다.\n",
    "\n",
    "img/\n",
    "- heartsping/\n",
    "  - heartsping0.jpg\n",
    "  - heartsping1.jpg\n",
    "  - heartsping2.jpg\n",
    "  - ...\n",
    "- shashaping/\n",
    "  - shashaping0.jpg\n",
    "  - shashaping1.jpg\n",
    "  - shashaping2.jpg\n",
    "  - ...\n",
    "- fluffyping/\n",
    "  - fluffyping0.jpg\n",
    "  - fluffyping1.jpg\n",
    "  - fluffyping2.jpg\n",
    "  - ...\n",
    "- jellyping/\n",
    "  - jellyping0.jpg\n",
    "  - jellyping1.jpg\n",
    "  - jellyping2.jpg\n",
    "  - ...\n",
    "- donutping/\n",
    "  - donutping0.jpg\n",
    "  - donutping1.jpg\n",
    "  - donutping2.jpg\n",
    "  - ...\n",
    "- puffping/\n",
    "  - puffping0.jpg\n",
    "  - puffping1.jpg\n",
    "  - puffping2.jpg\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 라이브러리를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 변환하고 전처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory (where your notebook is running)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Construct the path to the 'img' folder (relative path)\n",
    "data_dir = os.path.join(current_dir, 'img')\n",
    "\n",
    "# Define transformations (resize, convert to tensor, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
    "    transforms.ToTensor(),          # Convert images to tensors (PyTorch format)\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load dataset from 'img' folder using ImageFolder\n",
    "dataset = torchvision.datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Split dataset into training (80%) and validation (20%) sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for batch processing during training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN 모델을 구축한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=93312, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)  # Input channels: 3 (RGB), Output channels: 16\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 54 * 54, 128)      # Adjust the size based on input dimensions after pooling.\n",
    "        self.fc2 = nn.Linear(128, len(dataset.classes))  # Output size: number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 54 * 54)  # Flatten the tensor for fully connected layers.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model and move it to GPU if available.\n",
    "model = SimpleCNN()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실 함수와 옵티마이저 설정하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification tasks.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate of 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 학습시키기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimcj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.7319\n",
      "Epoch [2/10], Loss: 0.7570\n",
      "Epoch [3/10], Loss: 0.3092\n",
      "Epoch [4/10], Loss: 0.1355\n",
      "Epoch [5/10], Loss: 0.0535\n",
      "Epoch [6/10], Loss: 0.0297\n",
      "Epoch [7/10], Loss: 0.0192\n",
      "Epoch [8/10], Loss: 0.0189\n",
      "Epoch [9/10], Loss: 0.0141\n",
      "Epoch [10/10], Loss: 0.0166\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()            # Zero the parameter gradients.\n",
    "        outputs = model(inputs)          # Forward pass.\n",
    "        loss = criterion(outputs, labels) # Compute loss.\n",
    "        loss.backward()                  # Backward pass (compute gradients).\n",
    "        optimizer.step()                 # Update weights.\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 성능 평가하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 86.36%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation during evaluation.\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)   # Get class with highest score.\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on validation set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 저장하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'simple_cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)  # Input channels: 3 (RGB), Output channels: 16\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 54 * 54, 128)      # Adjust based on input dimensions after pooling.\n",
    "        self.fc2 = nn.Linear(128, len(dataset.classes))  # Output size: number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 54 * 54)  # Flatten for fully connected layers.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimcj\\AppData\\Local\\Temp\\ipykernel_14012\\891987856.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('simple_cnn.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=93312, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your trained model\n",
    "model = SimpleCNN()  # Ensure SimpleCNN is defined before this line\n",
    "model.load_state_dict(torch.load('simple_cnn.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Now proceed with preprocessing and making predictions on new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define transformations (same as during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match input size of your network\n",
    "    transforms.ToTensor(),          # Convert image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load and preprocess an image from the same directory\n",
    "img_path = 'test_img3.jpg'  # Replace with your actual image filename\n",
    "image = Image.open(img_path)\n",
    "\n",
    "# Apply transformations\n",
    "image = transform(image)\n",
    "\n",
    "# Add a batch dimension (as models expect batches of inputs)\n",
    "image = image.unsqueeze(0)  # Shape: [1, 3, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Probabilities:\n",
      "Class 0 (donutping): 0.0000\n",
      "Class 1 (fluffyping): 1.0000\n",
      "Class 2 (heartsping): 0.0000\n",
      "Class 3 (jellyping): 0.0000\n",
      "Class 4 (puffping): 0.0000\n",
      "Class 5 (shashaping): 0.0000\n",
      "\n",
      "Predicted Class: fluffyping with probability 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimcj\\AppData\\Local\\Temp\\ipykernel_14012\\1886686885.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('simple_cnn.pth'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load your trained model\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load('simple_cnn.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Define transformations (same as during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load and preprocess an image from the same directory\n",
    "img_path = 'test_img3.jpg'  # Replace with your actual image filename\n",
    "image = Image.open(img_path)\n",
    "image = transform(image)\n",
    "image = image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Move image and model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "image = image.to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(image)  # Raw model outputs (logits)\n",
    "\n",
    "# Apply softmax to get probabilities for each class\n",
    "probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "# Print probabilities for all classes\n",
    "print(\"Class Probabilities:\")\n",
    "for i, prob in enumerate(probabilities[0]):\n",
    "    print(f'Class {i} ({dataset.classes[i]}): {prob.item():.4f}')\n",
    "\n",
    "# Get the predicted class (the one with the highest probability)\n",
    "_, predicted_class_idx = torch.max(probabilities, 1)\n",
    "predicted_label = dataset.classes[predicted_class_idx.item()]\n",
    "\n",
    "# Print the final prediction result\n",
    "print(f'\\nPredicted Class: {predicted_label} with probability {probabilities[0][predicted_class_idx].item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
